from flask import Flask, render_template, jsonify, request, make_response, send_from_directory
import pandas as pd
import numpy as np
import os
import io
import json
import secrets
import string
from utils.tmap_utils import create_matrix_from_locations
from utils.report_generator import generate_standalone_route_html
from dotenv import load_dotenv
from urllib.parse import urlencode, urlparse, parse_qs

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = Flask(__name__)

# í”„ë¡œì íŠ¸ ê¸°ë°˜ íŒŒì¼ ê²½ë¡œ ì„¤ì •
BASE_PROJECTS_DIR = 'projects'

def _sanitize_project_id(raw: str | None) -> str:
    """í”„ë¡œì íŠ¸ IDë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì— ì•ˆì „í•˜ë„ë¡ ì •ê·œí™”í•©ë‹ˆë‹¤."""
    default_pid = 'default'
    if not raw:
        return default_pid
    # ì˜ìˆ«ì, í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
    import re
    pid = re.sub(r'[^A-Za-z0-9_-]', '', str(raw))[:50]
    return pid if pid else default_pid

def get_project_id() -> str:
    """ìš”ì²­ì—ì„œ projectIdë¥¼ ì¶”ì¶œ(ì¿¼ë¦¬/í—¤ë”/ì¿ í‚¤)í•˜ê³  ê¸°ë³¸ê°’ì€ 'default'."""
    try:
        pid = request.args.get('projectId') or request.headers.get('X-Project-Id') or request.cookies.get('projectId')
    except Exception:
        pid = None
    return _sanitize_project_id(pid)


def generate_short_id(length: int = 4) -> str:
    """ëŒ€ë¬¸ì + ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ì§€ì • ê¸¸ì´ì˜ ì•ˆì „í•œ ëœë¤ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ê¸°ë³¸ ê¸¸ì´ëŠ” 4ì…ë‹ˆë‹¤ (ì˜ˆ: 'A3Z7'). ì†Œë¬¸ìëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    alphabet = string.ascii_uppercase + string.digits
    return ''.join(secrets.choice(alphabet) for _ in range(length))

def ensure_project_dir(project_id: str) -> str:
    """í”„ë¡œì íŠ¸ ë””ë ‰í„°ë¦¬ë¥¼ ë³´ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    proj_dir = os.path.join(BASE_PROJECTS_DIR, project_id)
    os.makedirs(proj_dir, exist_ok=True)
    return proj_dir

def project_path(filename: str, project_id: str | None = None) -> str:
    """í”„ë¡œì íŠ¸ ì „ìš© íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    pid = project_id or get_project_id()
    proj_dir = ensure_project_dir(pid)
    return os.path.join(proj_dir, filename)

def migrate_root_files_to_default():
    """ë£¨íŠ¸ì— ìˆëŠ” ê¸°ì¡´ íŒŒì¼ë“¤ì„ projects/defaultë¡œ ì´ë™(ìµœì´ˆ 1íšŒ).
    ì´ë¯¸ ëŒ€ìƒ ê²½ë¡œì— ìˆìœ¼ë©´ ì´ë™í•˜ì§€ ì•ŠìŒ.
    """
    default_dir = ensure_project_dir('default')
    candidates = [
        'locations.csv',
        'time_matrix.csv',
        'distance_matrix.csv',
        'optimization_routes.csv',
        'optimization_summary.csv',
        'generated_routes.json',
        'route_metadata.json'
    ]
    for name in candidates:
        src = os.path.join(os.getcwd(), name)
        dst = os.path.join(default_dir, name)
        try:
            if os.path.exists(src) and not os.path.exists(dst):
                # ì´ë™(ì›ìì  rename ì‹œë„, ë‹¤ë¥¸ íŒŒí‹°ì…˜ì´ë©´ copy í›„ remove)
                try:
                    os.replace(src, dst)
                except Exception:
                    import shutil
                    shutil.copy2(src, dst)
                    os.remove(src)
                print(f"ğŸ“¦ Migrated '{name}' -> projects/default/{name}")
        except Exception as e:
            print(f"âš ï¸ Migration failed for {name}: {e}")

# ì•± ì‹œì‘ ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰
migrate_root_files_to_default()

# í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ/ìƒì„± API
@app.route('/api/projects', methods=['GET'])
def list_projects():
    try:
        # ê¸°ë³¸ í”„ë¡œì íŠ¸ ë””ë ‰í„°ë¦¬ ë³´ì¥
        ensure_project_dir('default')
        projects = []
        if not os.path.exists(BASE_PROJECTS_DIR):
            os.makedirs(BASE_PROJECTS_DIR, exist_ok=True)
        for name in os.listdir(BASE_PROJECTS_DIR):
            proj_dir = os.path.join(BASE_PROJECTS_DIR, name)
            if not os.path.isdir(proj_dir):
                continue
            loc_path = os.path.join(proj_dir, 'locations.csv')
            has_locations = os.path.exists(loc_path)
            count = 0
            try:
                if has_locations:
                    df = pd.read_csv(loc_path, encoding='utf-8-sig')
                    count = len(df)
            except Exception:
                pass
            projects.append({
                'id': name,
                'has_locations': has_locations,
                'location_count': int(count)
            })
        # ì´ë¦„ ê¸°ì¤€ ì •ë ¬(ê¸°ë³¸ default ìš°ì„ )
        projects.sort(key=lambda x: (x['id'] != 'default', x['id']))
        return jsonify({'projects': projects})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/projects', methods=['POST'])
def create_project():
    try:
        data = request.get_json(silent=True) or {}
        raw_id = data.get('projectId') or data.get('id')
        pid = _sanitize_project_id(raw_id)
        if not pid:
            return jsonify({'error': 'Invalid projectId'}), 400
        proj_dir = os.path.join(BASE_PROJECTS_DIR, pid)
        if os.path.exists(proj_dir):
            return jsonify({'error': 'Project already exists'}), 409
        os.makedirs(proj_dir, exist_ok=True)
        # ì´ˆê¸° locations.csv ìƒì„±(Depot 1ê°œ) - idëŠ” ë¬¸ìì—´ë¡œ ì €ì¥
        df = pd.DataFrame([
            {'id': '1', 'name': 'Depot', 'lon': 126.9779, 'lat': 37.5547, 'demand': 0}
        ], columns=['id', 'name', 'lon', 'lat', 'demand'])
        df.to_csv(os.path.join(proj_dir, 'locations.csv'), index=False, encoding='utf-8-sig')
        return jsonify({'created': True, 'id': pid}), 201
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def load_data(project_id: str | None = None):
    """CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. í•œê¸€ ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤."""
    locations_file = project_path('locations.csv', project_id)
    if os.path.exists(locations_file):
        # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
        encodings_to_try = ['utf-8', 'utf-8-sig', 'euc-kr', 'cp949']
        
        for encoding in encodings_to_try:
            try:
                df = pd.read_csv(locations_file, encoding=encoding)
                # ì—´ ì´ë¦„ ê³µë°± ì œê±° ë° ë¶ˆí•„ìš” ì—´ ì œê±°
                df.columns = df.columns.str.strip()
                df = df.loc[:, ~df.columns.str.startswith('Unnamed')]

                # ì¤‘ë³µ ID ì»¬ëŸ¼(id.1 ë“±) ì²˜ë¦¬
                if 'id.1' in df.columns:
                    if 'id' not in df.columns:
                        df['id'] = ''
                    df['id'] = df['id'].astype(str).replace({'nan': '', 'None': ''}).fillna('')
                    df['id.1'] = df['id.1'].astype(str).replace({'nan': '', 'None': ''}).fillna('')
                    empty_mask = df['id'].str.strip() == ''
                    df.loc[empty_mask, 'id'] = df.loc[empty_mask, 'id.1']
                    df = df.drop(columns=['id.1'])

                # ê¸°ëŒ€í•˜ëŠ” ì—´ë§Œ ìœ ì§€í•˜ê³  ë¶€ì¡±í•œ ì—´ì€ ì±„ì›Œë„£ê¸°
                expected_columns = ['id', 'name', 'lon', 'lat', 'demand']
                for col in expected_columns:
                    if col not in df.columns:
                        df[col] = np.nan
                df = df[expected_columns]

                # ë°ì´í„° íƒ€ì… ì •ê·œí™”
                df['id'] = df['id'].fillna('').astype(str)
                if 'name' in df.columns:
                    df['name'] = df['name'].fillna('').astype(str)
                if 'demand' in df.columns:
                    df['demand'] = pd.to_numeric(df['demand'], errors='coerce').fillna(0).astype(int)
                for coord_col in ('lon', 'lat'):
                    if coord_col in df.columns:
                        df[coord_col] = pd.to_numeric(df[coord_col], errors='coerce')

                # ID ì¤‘ë³µ/ë¹ˆê°’ ì²˜ë¦¬: ìµœì´ˆ ë¡œë“œ ì‹œì—ë„ ì¼ê´€ëœ ê·œì¹™ ì ìš©
                df['id'] = df['id'].fillna('').astype(str).str.strip()
                observed_ids: set[str] = set()
                empty_assigned = 0
                duplicate_sources: list[str] = []
                modified_ids = False

                for idx, current_id in df['id'].items():
                    if not current_id:
                        # ë¹ˆê°’ â†’ Depot(ì²« í–‰)ì´ë©´ '1', ë‚˜ë¨¸ì§€ëŠ” ìƒˆ ID ë¶€ì—¬
                        if idx == 0 and '1' not in observed_ids:
                            new_id = '1'
                        else:
                            new_id = None
                            attempts = 0
                            while attempts < 20:
                                candidate = generate_short_id(4)
                                if candidate not in observed_ids:
                                    new_id = candidate
                                    break
                                attempts += 1
                            if new_id is None:
                                # ì˜ˆì™¸ì ìœ¼ë¡œ ëª¨ë“  í›„ë³´ê°€ ê²¹ì¹˜ë©´ UUID fallback
                                import uuid
                                new_id = uuid.uuid4().hex[:4].upper()
                        df.at[idx, 'id'] = new_id
                        observed_ids.add(new_id)
                        empty_assigned += 1
                        modified_ids = True
                        continue

                    if current_id in observed_ids:
                        # ì¤‘ë³µ â†’ ìƒˆë¡œìš´ IDë¡œ ì¹˜í™˜í•˜ë˜, ê¸°ì¡´ ê°’ ê¸°ë¡
                        duplicate_sources.append(current_id)
                        new_id = None
                        attempts = 0
                        while attempts < 20:
                            candidate = generate_short_id(4)
                            if candidate not in observed_ids:
                                new_id = candidate
                                break
                            attempts += 1
                        if new_id is None:
                            import uuid
                            new_id = uuid.uuid4().hex[:4].upper()
                        df.at[idx, 'id'] = new_id
                        observed_ids.add(new_id)
                        modified_ids = True
                    else:
                        observed_ids.add(current_id)

                if duplicate_sources:
                    print(f"âš ï¸ locations.csvì—ì„œ ì¤‘ë³µëœ IDë¥¼ ê°ì§€í•˜ì—¬ ì¬í• ë‹¹í–ˆìŠµë‹ˆë‹¤: {duplicate_sources}")
                if empty_assigned:
                    print(f"â„¹ï¸ locations.csvì— ë¹„ì–´ ìˆëŠ” ID {empty_assigned}ê°œì— ëœë¤ ê°’ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤.")
                if modified_ids:
                    try:
                        save_data(df, project_id)
                        print("ğŸ’¾ ID ì •ê·œí™” ê²°ê³¼ë¥¼ locations.csvì— ì¦‰ì‹œ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.")
                    except Exception as persist_error:
                        print(f"âš ï¸ ID ì •ê·œí™” ë‚´ìš©ì„ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {persist_error}")

                print(f"CSV íŒŒì¼ì„ {encoding} ì¸ì½”ë”©ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                return df
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception as e:
                print(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ({encoding}): {e}")
                continue
        
        # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì²˜ë¦¬
        print("ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
    
    return pd.DataFrame(columns=['id', 'name', 'lon', 'lat', 'demand'])

def save_data(df, project_id: str | None = None):
    """ë°ì´í„°ë¥¼ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤. UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥í•˜ì—¬ í•œê¸€ ê¹¨ì§ì„ ë°©ì§€í•©ë‹ˆë‹¤."""
    try:
        locations_file = project_path('locations.csv', project_id)
        # UTF-8 BOMê³¼ í•¨ê»˜ ì €ì¥í•˜ì—¬ Excelì—ì„œë„ í•œê¸€ì´ ì •ìƒ í‘œì‹œë˜ë„ë¡ í•¨
        df.to_csv(locations_file, index=False, encoding='utf-8-sig')
        print(f"ë°ì´í„°ë¥¼ UTF-8-SIG ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¸ì½”ë”©ìœ¼ë¡œ ì¬ì‹œë„
        df.to_csv(locations_file, index=False, encoding='utf-8')

def save_optimization_result_to_csv(result, vehicle_count, vehicle_capacity, project_id: str | None = None):
    """ìµœì í™” ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    pid = project_id or get_project_id()
    # ìš”ì•½ ì •ë³´
    summary_data = {
        'Total_Distance_m': [result['total_distance']],
        'Total_Time_s': [result.get('total_time', 0)],
        'Total_Load': [result['total_load']],
        'Objective_Value': [result['objective']],
        'Vehicle_Count': [vehicle_count],
        'Vehicle_Capacity': [vehicle_capacity]
    }
    
    # ìš”ì•½ ì •ë³´ CSV ì €ì¥ (UTF-8 BOMìœ¼ë¡œ Excel í˜¸í™˜ì„± í™•ë³´)
    summary_df = pd.DataFrame(summary_data)
    try:
        summary_df.to_csv(project_path('optimization_summary.csv', pid), index=False, encoding='utf-8-sig')
        print("ìš”ì•½ íŒŒì¼ì„ UTF-8-SIG ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ìš”ì•½ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        summary_df.to_csv(project_path('optimization_summary.csv', pid), index=False, encoding='utf-8')
    
    # ìƒì„¸ ê²½ë¡œ ì •ë³´ CSV ì €ì¥ (UTF-8 BOMìœ¼ë¡œ Excel í˜¸í™˜ì„± í™•ë³´)
    route_details = []
    # per-route ì´ì „ ëˆ„ì ê°’ì„ ì¶”ì í•˜ì—¬ ì •ë¥˜ì¥ë³„ Load(ìˆ˜ìš”)ë¥¼ ê³„ì‚°
    # load locations map for id lookup
    try:
        loc_df_for_ids = pd.read_csv(project_path('locations.csv', pid), encoding='utf-8-sig')
    except Exception:
        try:
            loc_df_for_ids = pd.read_csv(project_path('locations.csv', pid), encoding='utf-8')
        except Exception:
            loc_df_for_ids = pd.DataFrame()

    def _normalize_name_for_lookup(s: str | None) -> str:
        if s is None:
            return ''
        try:
            return ' '.join(str(s).strip().lower().split())
        except Exception:
            return str(s).strip().lower()

    name_to_id = {}
    if not loc_df_for_ids.empty and 'name' in loc_df_for_ids.columns and 'id' in loc_df_for_ids.columns:
        for _, lr in loc_df_for_ids.iterrows():
            try:
                name_to_id[_normalize_name_for_lookup(lr['name'])] = str(lr['id'])
            except Exception:
                continue

    for route in result['routes']:
        prev_cum = 0
        for i, waypoint in enumerate(route['waypoints']):
            cumulative = int(waypoint.get('load', 0) or 0)
            # depotì€ ìˆ˜ìš” 0, ê·¸ ì™¸ëŠ” (í˜„ì¬ ëˆ„ì  - ì´ì „ ëˆ„ì )
            load_delta = 0 if str(waypoint.get('type')) == 'depot' else max(0, cumulative - prev_cum)
            prev_cum = cumulative
            route_details.append({
                'Vehicle_ID': route['vehicle_id'] + 1,
                'Route_Distance_m': waypoint.get('cumulative_distance', 0),
                'Route_Time_s': waypoint.get('cumulative_time', 0),
                'Route_Load': route['load'],
                'Stop_Order': i + 1,
                'Location_Name': waypoint['name'],
                'Location_ID': name_to_id.get(_normalize_name_for_lookup(waypoint.get('name')), ''),
                'Location_Type': waypoint['type'],
                'Load': load_delta,
                'Cumulative_Load': cumulative
            })
    
    routes_df = pd.DataFrame(route_details)
    try:
        routes_df.to_csv(project_path('optimization_routes.csv', pid), index=False, encoding='utf-8-sig')
        print("ìƒì„¸ ê²½ë¡œ íŒŒì¼ì„ UTF-8-SIG ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ìƒì„¸ ê²½ë¡œ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        routes_df.to_csv(project_path('optimization_routes.csv', pid), index=False, encoding='utf-8')
    
    print(f"ìµœì í™” ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print(f"- ìš”ì•½: {project_path('optimization_summary.csv', pid)}")
    print(f"- ìƒì„¸ ê²½ë¡œ: {project_path('optimization_routes.csv', pid)}")

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    mapbox_token = os.getenv('MAPBOX_ACCESS_TOKEN')
    return render_template('index.html', mapbox_token=mapbox_token)

@app.route('/api/locations', methods=['GET'])
def get_locations():
    """ëª¨ë“  ìœ„ì¹˜ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    pid = get_project_id()
    df = load_data(pid)
    # ê²°ì¸¡ì¹˜(NaN)ë¥¼ Noneìœ¼ë¡œ ë³€í™˜í•˜ê³  ê·¸ëŒ€ë¡œì˜ ìˆœì„œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
    records = df.where(pd.notnull(df), None).to_dict(orient='records')
    return jsonify(records)

@app.route('/api/locations', methods=['POST'])
def add_location():
    """ìƒˆë¡œìš´ ìœ„ì¹˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        data = request.json
        pid = get_project_id()
        df = load_data(pid)
        # ì•ˆì „í•œ 6ìë¦¬ ì˜ìˆ«ì ID ìƒì„±(ì¶©ëŒ ì²´í¬ í¬í•¨)
        existing_ids = set(df['id'].astype(str).tolist()) if not df.empty and 'id' in df.columns else set()
        # ë§Œì•½ í…Œì´ë¸”ì´ ë¹„ì–´ìˆë‹¤ë©´ Depot(ì´ˆê¸° ìƒì„±)ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜, ìƒˆ í”„ë¡œì íŠ¸ì¼ ìˆ˜ ìˆìŒ.
        # ìš”ì²­ìœ¼ë¡œ íŠ¹ì • idê°€ ì œê³µëœ ê²½ìš°(ë“œë¬¸ ì¼€ì´ìŠ¤) ìš°ì„  ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì²˜ë¦¬
        if data and data.get('id'):
            candidate = str(data.get('id'))
            if candidate in existing_ids:
                return jsonify({'error': 'ID already exists'}), 409
            new_id = candidate
        else:
            # ì¶©ëŒì´ ê±°ì˜ ë°œìƒí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ ì¬ì‹œë„ë¡œ ì¶©ë¶„
            attempts = 0
            new_id = None
            while attempts < 10:
                # ê¸°ë³¸ì ìœ¼ë¡œ 4ìë¦¬(ëŒ€ë¬¸ì+ìˆ«ì) IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                candidate = generate_short_id(4)
                if candidate not in existing_ids:
                    new_id = candidate
                    break
                attempts += 1
            if new_id is None:
                # ë“œë¬¼ê²Œ ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´ UUIDfallback
                import uuid
                new_id = uuid.uuid4().hex[:8]

        new_location = {
            'id': str(new_id),
            'name': str(data['name']),
            'lon': float(data['lon']),
            'lat': float(data['lat']),
            'demand': int(data['demand']) if data.get('demand') else 0
        }

        df = pd.concat([df, pd.DataFrame([new_location])], ignore_index=True)
        save_data(df, pid)
        return jsonify(new_location), 201
    except Exception as e:
        print(f"Error in add_location: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/locations/<location_id>', methods=['PUT'])
def update_location(location_id):
    """ê¸°ì¡´ ìœ„ì¹˜ ì •ë³´ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    try:
        data = request.json
        pid = get_project_id()
        df = load_data(pid)
        
        # IDs are stored as strings; ensure comparison as string
        str_id = str(location_id)
        if str_id in df['id'].values:
            idx = df[df['id'] == str_id].index[0]
            # ì´ë¦„ ì œê³µ ì‹œì—ë§Œ ê°±ì‹ 
            if 'name' in data and data.get('name') is not None:
                df.loc[idx, 'name'] = str(data.get('name', df.loc[idx, 'name']))
            
            # Check if this is the first location (depot) - force demand to 0
            # Depot ê²°ì •: íŒŒì¼ìƒ ì²« ë²ˆì§¸ í–‰(íŒŒì¼ ìƒì„± ì‹œ Depotì„ ì œì¼ ì²˜ìŒì— ë‘¡ë‹ˆë‹¤)
            try:
                first_id = df.iloc[0]['id']
            except Exception:
                first_id = df['id'].min() if 'id' in df.columns else None
            is_depot = df.loc[idx, 'id'] == first_id
            if is_depot:
                df.loc[idx, 'demand'] = 0
            else:
                if 'demand' in data and data.get('demand') is not None:
                    df.loc[idx, 'demand'] = int(data.get('demand', df.loc[idx, 'demand']))

            # ìœ„ê²½ë„ ì „ë‹¬ ì‹œ ê°±ì‹ (ë“œë˜ê·¸ ì•¤ ë“œë¡­ ë°˜ì˜)
            if 'lon' in data and data.get('lon') is not None:
                try:
                    df.loc[idx, 'lon'] = float(data.get('lon'))
                except (ValueError, TypeError):
                    pass
            if 'lat' in data and data.get('lat') is not None:
                try:
                    df.loc[idx, 'lat'] = float(data.get('lat'))
                except (ValueError, TypeError):
                    pass
            
            save_data(df, pid)

            # ìœ„ì¹˜ ë³€ê²½ì— ë”°ë¥¸ ìºì‹œ/ê²°ê³¼ íŒŒì¼ ë¬´íš¨í™” ì²˜ë¦¬
            try:
                cache_files = [
                    project_path('time_matrix.csv', pid),
                    project_path('distance_matrix.csv', pid),
                    project_path('optimization_routes.csv', pid),
                    project_path('optimization_summary.csv', pid),
                    project_path('generated_routes.json', pid),
                    project_path('route_metadata.json', pid)
                ]
                for f in cache_files:
                    if os.path.exists(f):
                        os.remove(f)
                        print(f"âš ï¸ ìœ„ì¹˜ ë³€ê²½ìœ¼ë¡œ ìºì‹œ/ê²°ê³¼ íŒŒì¼ ì‚­ì œ: {f}")
            except Exception as ce:
                print(f"ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨(update_location): {ce}")

            return jsonify(df.loc[idx].to_dict())
        return jsonify({'error': 'Location not found'}), 404
    except Exception as e:
        print(f"Error in update_location: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/locations/<location_id>', methods=['DELETE'])
def delete_location(location_id):
    """ìœ„ì¹˜ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    pid = get_project_id()
    df = load_data(pid)
    str_id = str(location_id)
    if str_id in df['id'].values:
        df = df[df['id'] != str_id]
        save_data(df, pid)
        return '', 204
    return jsonify({'error': 'Location not found'}), 404


@app.route('/api/optimize-settings', methods=['GET'])
def get_optimize_settings():
    """í”„ë¡œì íŠ¸ë³„ ìµœì í™” íŒì—… ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì„¤ì • íŒŒì¼ì´ ì—†ìœ¼ë©´ exists=Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        pid = get_project_id()
        settings_path = project_path('optimization_settings.json', pid)
        if os.path.exists(settings_path):
            try:
                with open(settings_path, 'r', encoding='utf-8') as fh:
                    data = json.load(fh)
                return jsonify({'exists': True, 'settings': data})
            except Exception as e:
                return jsonify({'error': f'Failed to load settings: {e}'}), 500
        return jsonify({'exists': False, 'settings': {}})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/optimize-settings', methods=['POST'])
def save_optimize_settings():
    """ìš”ì²­ ë°”ë””(JSON)ë¥¼ ë°›ì•„ í”„ë¡œì íŠ¸ í´ë”ì— optimization_settings.jsonìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        pid = get_project_id()
        payload = request.get_json(silent=True) or {}
        settings_path = project_path('optimization_settings.json', pid)
        # ì €ì¥: UTF-8ë¡œ human-readableí•˜ê²Œ
        with open(settings_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, ensure_ascii=False, indent=2)
        return jsonify({'saved': True}), 201
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/download')
def download_file():
    """CSV íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. UTF-8 BOMìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ Excelì—ì„œë„ í•œê¸€ì´ ì •ìƒ í‘œì‹œë©ë‹ˆë‹¤."""
    pid = get_project_id()
    df = load_data(pid)
    
    # UTF-8 BOMìœ¼ë¡œ ì¸ì½”ë”©ëœ CSV ìƒì„±
    output = io.StringIO()
    df.to_csv(output, index=False, encoding='utf-8-sig')
    output.seek(0)
    
    # UTF-8 BOM ì¶”ê°€ (Excel í˜¸í™˜ì„±)
    csv_content = '\ufeff' + output.getvalue()
    
    response = make_response(csv_content.encode('utf-8-sig'))
    response.headers["Content-Disposition"] = "attachment; filename=locations.csv"
    response.headers["Content-type"] = "text/csv; charset=utf-8-sig"
    return response

@app.route('/download-demand-form')
def download_demand_form():
    """apps í´ë”ì— ìˆëŠ” demand_form.csvë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # ì•± ë£¨íŠ¸ ê¸°ì¤€ íŒŒì¼ ì œê³µ
        directory = os.path.abspath(os.path.dirname(__file__))
        return send_from_directory(directory, 'demand_form.csv', as_attachment=True, download_name='demand_form.csv')
    except Exception as e:
        return jsonify({ 'error': f'Failed to download form: {str(e)}' }), 500

@app.route('/upload', methods=['POST'])
def upload_file():
    """CSV íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    if 'file' not in request.files:
        return 'No file part', 400
    file = request.files['file']
    if file.filename == '':
        return 'No selected file', 400
    if file and file.filename.endswith('.csv'):
        pid = get_project_id()
        # ì—…ë¡œë“œëœ íŒŒì¼ì˜ ì¸ì½”ë”©ì„ ìë™ ê°ì§€í•˜ì—¬ ì½ê¸°
        encodings_to_try = ['utf-8', 'utf-8-sig', 'euc-kr', 'cp949']
        df = None
        
        for encoding in encodings_to_try:
            try:
                # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
                file.seek(0)
                df = pd.read_csv(file, encoding=encoding)
                print(f"ì—…ë¡œë“œ íŒŒì¼ì„ {encoding} ì¸ì½”ë”©ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
                break
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({encoding}): {e}")
                continue
        
        if df is None:
            return 'File encoding not supported. Please use UTF-8, EUC-KR, or CP949 encoding.', 400
            
        # ì»¬ëŸ¼ ì •ê·œí™”: ê³µë°± ì œê±° ë° ì†Œë¬¸ì ë³€í™˜, ì¼ë¶€ ë™ì˜ì–´ ë§¤í•‘
        try:
            original_cols = list(df.columns)
            # ì¹¼ëŸ¼ëª…ì—ì„œ BOM ë˜ëŠ” ë³´ì´ì§€ ì•ŠëŠ” íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì†Œë¬¸ìí™”
            def clean_col(c):
                s = str(c)
                s = s.replace('\ufeff', '')  # UTF-8 BOM ì œê±°
                s = s.replace('\u200b', '')  # zero-width space ì œê±°
                return s.strip().lower()

            normalized = {col: clean_col(col) for col in df.columns}
            df.rename(columns=normalized, inplace=True)
            # ë™ì˜ì–´ ì²˜ë¦¬
            synonym_map = {}
            if 'longitude' in df.columns and 'lon' not in df.columns:
                synonym_map['longitude'] = 'lon'
            if 'latitude' in df.columns and 'lat' not in df.columns:
                synonym_map['latitude'] = 'lat'
            if 'x' in df.columns and 'lon' not in df.columns:
                synonym_map['x'] = 'lon'
            if 'y' in df.columns and 'lat' not in df.columns:
                synonym_map['y'] = 'lat'
            if synonym_map:
                df.rename(columns=synonym_map, inplace=True)

            required_cols = ['id', 'name', 'lon', 'lat', 'demand']
            if not all(col in df.columns for col in required_cols):
                return 'Invalid CSV format. Required columns: id, name, lon, lat, demand', 400

            # íƒ€ì… ë³´ì •(ê°€ëŠ¥í•  ë•Œë§Œ)
            # lon/latì€ ìˆ«ìì—¬ì•¼ í•˜ë©° ìœ íš¨í•œ ë²”ìœ„ì¸ì§€ ê²€ì‚¬
            df['lon'] = pd.to_numeric(df['lon'], errors='coerce')
            df['lat'] = pd.to_numeric(df['lat'], errors='coerce')
            df['demand'] = pd.to_numeric(df['demand'], errors='coerce').fillna(0)

            # ê¸°ë³¸ì ì¸ ê²°ì¸¡/íƒ€ì… ê²€ì‚¬: í•„ìˆ˜ ì¢Œí‘œê°€ ì—†ëŠ” í–‰ì€ ì˜¤ë¥˜ë¡œ ì²˜ë¦¬
            bad_coord = df[df['lon'].isna() | df['lat'].isna()]
            if not bad_coord.empty:
                rows = bad_coord.index.tolist()
                return f'Invalid coordinates in rows: {rows}', 400

            # ì¢Œí‘œ ë²”ìœ„ ì²´í¬
            invalid_bounds = df[(df['lon'] < -180) | (df['lon'] > 180) | (df['lat'] < -90) | (df['lat'] > 90)]
            if not invalid_bounds.empty:
                return f'Latitude/longitude out of range for rows: {invalid_bounds.index.tolist()}', 400

            # demandëŠ” ìŒìˆ˜ì¼ ìˆ˜ ì—†ìŒ
            if (df['demand'] < 0).any():
                return f'Demand must be non-negative for all rows', 400

            # idëŠ” ë¬¸ìì—´ë¡œ ì •ê·œí™” (ë¹ˆê°’ì€ ë¹ˆ ë¬¸ìì—´)
            df['id'] = df['id'].fillna('').astype(str).str.strip()

            # ë¹ˆ idì— ëŒ€í•´ì„œëŠ” 4ìë¦¬ ëŒ€ë¬¸ì+ìˆ«ì IDë¥¼ ìƒì„±
            existing_ids = set(df['id'][df['id'] != ''].tolist())
            new_ids = []
            for idx, val in df['id'].items():
                if not val:
                    # ìƒì„± ì‹œ ì¶©ëŒì´ ì—†ì„ ë•Œê¹Œì§€ ì¬ì‹œë„
                    attempts = 0
                    candidate = None
                    while attempts < 20:
                        candidate = generate_short_id(4)
                        if candidate not in existing_ids:
                            existing_ids.add(candidate)
                            break
                        attempts += 1
                    if candidate is None:
                        import uuid
                        candidate = uuid.uuid4().hex[:4].upper()
                    df.at[idx, 'id'] = candidate
                    new_ids.append((idx, candidate))

            # ì¤‘ë³µ ID ê²€ì‚¬
            dupes = df[df['id'].duplicated(keep=False)]['id'].unique().tolist()
            if dupes:
                # ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ë©”ì‹œì§€ ë°˜í™˜
                return f'ì—…ë¡œë“œ ì‹¤íŒ¨: CSVì— ì¤‘ë³µëœ IDê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì¤‘ë³µ ID: {dupes}', 400

            # depot ë³´ì¥: id '1'ì´ ì—†ë‹¤ë©´ ì²« í–‰ì„ depotìœ¼ë¡œ ì„¤ì •
            if '1' not in df['id'].values:
                # first rowë¥¼ depotìœ¼ë¡œ ì„¤ì •
                df.iloc[0, df.columns.get_loc('id')] = '1'
                df.iloc[0, df.columns.get_loc('demand')] = 0

            # ëª¨ë“  idë¥¼ ë¬¸ìì—´ë¡œ ìœ ì§€
            df['id'] = df['id'].astype(str)

            # demandë¥¼ ì •ìˆ˜ë¡œ
            df['demand'] = df['demand'].astype(int)

        except Exception as norm_e:
            print(f"ì—…ë¡œë“œ ë°ì´í„° ì •ê·œí™” ì˜¤ë¥˜: {norm_e}")
            return f'Failed to normalize uploaded CSV: {norm_e}', 400

        # ì €ì¥
        save_data(df, pid)

        # ì—…ë¡œë“œë¡œ ì¸í•´ ì´ì „ ê³„ì‚°/ìºì‹œ íŒŒì¼ ë¬´íš¨í™”(ë°ì´í„° ì¼ê´€ì„±)
        cache_files = [
            project_path('time_matrix.csv', pid),
            project_path('distance_matrix.csv', pid),
            project_path('optimization_routes.csv', pid),
            project_path('optimization_summary.csv', pid),
            project_path('generated_routes.json', pid),
            project_path('route_metadata.json', pid)
        ]
        cleared = []
        for f in cache_files:
            try:
                if os.path.exists(f):
                    os.remove(f)
                    cleared.append(f)
                    print(f"âš ï¸ ì—…ë¡œë“œë¡œ ì¸í•´ ìºì‹œ/ê²°ê³¼ íŒŒì¼ ì‚­ì œ: {f}")
            except Exception as ce:
                print(f"ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {f} - {ce}")

        # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì‘ë‹µ ìœ ì§€(í”„ë¡ íŠ¸ ê¸°ì¡´ ë¡œì§ í˜¸í™˜)
        return 'File uploaded successfully', 200
    return 'Invalid file type', 400

@app.route('/api/create-matrix', methods=['POST'])
def create_matrix():
    """ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        data = request.json
        transport_mode = data.get('transportMode', 'car')
        metric = data.get('metric', 'Recommendation')
        pid = get_project_id()
        
        # ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        result = create_matrix_from_locations(
            transport_mode,
            metric,
            locations_file=project_path('locations.csv', pid),
            time_filename=project_path('time_matrix.csv', pid),
            distance_filename=project_path('distance_matrix.csv', pid)
        )
        
        if result['success']:
            return jsonify({
                'success': True,
                'message': result['message'],
                'time_matrix': result.get('time_matrix'),
                'distance_matrix': result.get('distance_matrix'),
                'locations': result['locations']
            })
        else:
            return jsonify({
                'success': False,
                'message': result['message']
            }), 500
            
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500

@app.route('/api/check-matrix-file', methods=['GET'])
def check_matrix_file():
    """ì‹œê°„ ë° ê±°ë¦¬ ë§¤íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    pid = get_project_id()
    time_matrix_file = project_path('time_matrix.csv', pid)
    distance_matrix_file = project_path('distance_matrix.csv', pid)
    
    # ë‘ íŒŒì¼ ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•¨
    exists = os.path.exists(time_matrix_file) and os.path.exists(distance_matrix_file)
    
    return jsonify({
        'exists': exists,
        'time_matrix_exists': os.path.exists(time_matrix_file),
        'distance_matrix_exists': os.path.exists(distance_matrix_file)
    })

@app.route('/api/optimize', methods=['POST'])
def optimize():
    """ìµœì í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        from utils.vrp_solver import solve_vrp
        pid = get_project_id()
        
        # ğŸ—‘ï¸ ê¸°ì¡´ ê²½ë¡œ ìºì‹œ íŒŒì¼ ì‚­ì œ (ìƒˆë¡œìš´ ìµœì í™” ì‹œì‘)
        route_cache_files = [project_path('generated_routes.json', pid), project_path('route_metadata.json', pid)]
        for cache_file in route_cache_files:
            if os.path.exists(cache_file):
                try:
                    os.remove(cache_file)
                    print(f"âœ… ê¸°ì¡´ ê²½ë¡œ ìºì‹œ ì‚­ì œ: {cache_file}")
                except Exception as e:
                    print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {cache_file} - {e}")
        
        data = request.json
        vehicle_count = data.get('vehicleCount', 1)
        vehicle_capacity = data.get('vehicleCapacity', 10)
        time_limit_sec = data.get('timeLimitSec', 60)
        
        # ëª©ì í•¨ìˆ˜ ì„¤ì • ì¶”ì¶œ (ê¸°ë³¸ê°’ ì„¤ì •)
        primary_objective = data.get('primaryObjective', 'distance')
        tiebreaker1 = data.get('tiebreaker1', 'none')
        tiebreaker2 = data.get('tiebreaker2', 'none')
        additional_objectives = data.get('additionalObjectives', [])
        
        # ë§¤íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        time_matrix_file = project_path('time_matrix.csv', pid)
        distance_matrix_file = project_path('distance_matrix.csv', pid)
        
        if not os.path.exists(time_matrix_file):
            return jsonify({
                'success': False,
                'message': 'Time matrix file not found. Please create matrix first.'
            }), 400
            
        if not os.path.exists(distance_matrix_file):
            return jsonify({
                'success': False,
                'message': 'Distance matrix file not found. Please create matrix first.'
            }), 400
        
        # locations.csv íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        locations_file = project_path('locations.csv', pid)
        if not os.path.exists(locations_file):
            return jsonify({
                'success': False,
                'message': 'Locations file not found. Please add locations first.'
            }), 400
        
        # ì‹œì‘/ì¢…ë£Œ ì˜µì…˜ (ììœ  ì¶œë°œ, depot ë„ì°© ê³ ì •) - ì„ íƒì 
        # ë¼ë””ì˜¤: routeMode = FREE_START_DEPOT_END | DEPOT_START_OPEN_END
        route_mode = data.get('routeMode', 'FREE_START_DEPOT_END')
        # êµ¬ë²„ì „ ì²´í¬ë°•ìŠ¤ í˜¸í™˜ ì²˜ë¦¬ (ìˆë‹¤ë©´ ìš°ì„ )
        start_anywhere = bool(data.get('startAnywhere', False))
        end_at_depot_only = bool(data.get('endAtDepotOnly', False))
        open_end = False
        if 'routeMode' in data or (not start_anywhere and not end_at_depot_only):
            # routeModeë¥¼ solver í”Œë˜ê·¸ë¡œ ë§¤í•‘
            if route_mode == 'FREE_START_DEPOT_END':
                start_anywhere = True
                end_at_depot_only = True
            elif route_mode == 'DEPOT_START_OPEN_END':
                start_anywhere = False
                end_at_depot_only = False
                open_end = True

        # VRP ìµœì í™” ì‹¤í–‰ (ìƒˆë¡œìš´ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©)
        result = solve_vrp(
            locations_csv_path=locations_file,
            time_matrix_path=time_matrix_file,
            distance_matrix_path=distance_matrix_file,
            vehicle_capacity=vehicle_capacity,
            num_vehicles=vehicle_count,
            time_limit_sec=time_limit_sec,
            primary_objective=primary_objective,
            tiebreaker1=tiebreaker1,
            tiebreaker2=tiebreaker2,
            additional_objectives=additional_objectives,
            start_anywhere=start_anywhere,
            end_at_depot_only=end_at_depot_only,
            open_end=open_end
        )
        
        if result['success']:
            # CSV íŒŒì¼ë¡œ ìµœì í™” ê²°ê³¼ ì €ì¥
            save_optimization_result_to_csv(result, vehicle_count, vehicle_capacity, pid)
            
            return jsonify({
                'success': True,
                'message': 'Optimization completed successfully',
                'vehicleCount': vehicle_count,
                'vehicleCapacity': vehicle_capacity,
                'routes': result['routes'],
                'objective': result['objective'],
                'total_distance': result['total_distance'],
                'total_time': result.get('total_time', 0),
                'total_load': result['total_load']
            })
        else:
            # ìƒì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì œê³µ
            error_message = result.get('error', 'Optimization failed')
            
            # ê²€ì¦ ì˜¤ë¥˜ì¸ ê²½ìš° ë” êµ¬ì²´ì ì¸ ë©”ì‹œì§€
            if 'validation_errors' in result:
                error_message = "ì…ë ¥ ë°ì´í„° ì˜¤ë¥˜:\n" + "\n".join([f"â€¢ {error}" for error in result['validation_errors']])
            
            # ì§„ë‹¨ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if 'diagnosis' in result:
                diagnosis = result['diagnosis']
                if diagnosis['type'] == 'capacity_constraint':
                    error_message = f"ìš©ëŸ‰ ë¶€ì¡± ì˜¤ë¥˜: ì´ ìˆ˜ìš”ê°€ ì´ ì°¨ëŸ‰ ìš©ëŸ‰ì„ ì´ˆê³¼í•©ë‹ˆë‹¤.\ní•´ê²°ë°©ë²•: ì°¨ëŸ‰ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ìš©ëŸ‰ì„ ì¦ê°€ì‹œì¼œì£¼ì„¸ìš”."
                elif diagnosis['type'] == 'individual_capacity':
                    error_message = f"ê°œë³„ ìš©ëŸ‰ ì´ˆê³¼: ì¼ë¶€ ìœ„ì¹˜ì˜ ìˆ˜ìš”ê°€ ì°¨ëŸ‰ ìš©ëŸ‰ì„ ì´ˆê³¼í•©ë‹ˆë‹¤.\ní•´ê²°ë°©ë²•: ì°¨ëŸ‰ ìš©ëŸ‰ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”."
                else:
                    error_message = diagnosis['message']
            
            return jsonify({
                'success': False,
                'message': error_message
            }), 400
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'ìµœì í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500

@app.route('/api/check-routes', methods=['GET'])
def check_routes():
    """optimization_routes.csvì™€ locations.csv íŒŒì¼ì˜ ì¡´ì¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    pid = get_project_id()
    routes_file = project_path('optimization_routes.csv', pid)
    locations_file = project_path('locations.csv', pid)
    
    try:
        # Routes íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(routes_file):
            return jsonify({
                'has_routes': False,
                'message': 'Routes file not found'
            })
        
        # Routes íŒŒì¼ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        if os.path.getsize(routes_file) <= 1:  # 1ë°”ì´íŠ¸ ì´í•˜ë©´ ë¹ˆ íŒŒì¼
            return jsonify({
                'has_routes': False,
                'message': 'Routes file is empty'
            })
            
        # Locations íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(locations_file):
            return jsonify({
                'has_routes': False,
                'message': 'Locations file not found'
            })
            
        # Locations íŒŒì¼ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        if os.path.getsize(locations_file) <= 1:
            return jsonify({
                'has_routes': False,
                'message': 'Locations file is empty'
            })
        
        # ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ OK
        return jsonify({
            'has_routes': True,
            'message': 'Both routes and locations files exist'
        })
            
    except Exception as e:
        return jsonify({
            'has_routes': False,
            'message': f'Error checking routes: {str(e)}'
        })

# Route Visualization ê´€ë ¨ ë¼ìš°íŠ¸ë“¤
from datetime import datetime
from utils.tmap_route import TmapRoute

@app.route('/route-visualization')
def route_visualization():
    """ë…ë¦½ì ì¸ route visualization í˜ì´ì§€"""
    pid = get_project_id()
    # ê¸°ì¡´ ìµœì í™” ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    route_data = {
        'routes': [],
        'total_distance': 'N/A',
        'total_duration': 'N/A', 
        'route_count': 0,
        'optimization_score': 'N/A'
    }
    
    # optimization_routes.csv íŒŒì¼ì´ ìˆìœ¼ë©´ ë°ì´í„° ë¡œë“œ
    try:
        if os.path.exists(project_path('optimization_routes.csv', pid)):
            routes_df = pd.read_csv(project_path('optimization_routes.csv', pid), encoding='utf-8-sig')
            route_data = convert_routes_df_to_visualization_data(routes_df, pid)
    except Exception as e:
        print(f"Route data loading error: {e}")
    
    mapbox_token = os.getenv('MAPBOX_ACCESS_TOKEN')
    return render_template('route_visualization.html', 
                         route_data=route_data, 
                         current_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                         mapbox_token=mapbox_token)

@app.route('/generate-route-html', methods=['POST'])
def generate_route_html():
    """ë¼ìš°íŠ¸ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ì™„ì „í•œ HTML íŒŒì¼ì„ ìƒì„±"""
    try:
        pid = get_project_id()
        request_data = request.get_json()
        
        # í˜„ì¬ ìµœì í™” ê²°ê³¼ ë¡œë“œ
        route_data = {
            'routes': [],
            'total_distance': 'N/A',
            'total_duration': 'N/A',
            'route_count': 0,
            'optimization_score': 'N/A'
        }
        
        if os.path.exists(project_path('optimization_routes.csv', pid)):
            routes_df = pd.read_csv(project_path('optimization_routes.csv', pid), encoding='utf-8-sig')
            route_data = convert_routes_df_to_visualization_data(routes_df, pid)
        
        # Per-project report path
        report_filename = 'route_report.html'
        report_path = project_path(report_filename, pid)

        # If report exists, serve it directly from project folder
        if os.path.exists(report_path):
            proj_dir = os.path.dirname(report_path)
            return send_from_directory(proj_dir, report_filename, mimetype='text/html')

        # Generate standalone HTML and save it into project folder
        html_content = generate_standalone_route_html(route_data)
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
        except Exception as e:
            print(f"Failed to write report for project {pid}: {e}")

        proj_dir = os.path.dirname(report_path)
        return send_from_directory(proj_dir, report_filename, mimetype='text/html')
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/generate-route-table-report', methods=['POST'])
def generate_route_table_report():
    """Generate the table-based route report (reads generated_routes.json per project) and return HTML."""
    try:
        pid = get_project_id()
        report_filename = 'route_table_report.html'
        report_path = project_path(report_filename, pid)

        # If a per-project report already exists, serve it directly (reuse)
        if os.path.exists(report_path):
            proj_dir = os.path.dirname(report_path)
            return send_from_directory(proj_dir, report_filename, mimetype='text/html')

        # Otherwise generate and save it
        try:
            from utils.report_generator import generate_route_table_report_html
            report_html = generate_route_table_report_html(project_id=pid)
        except Exception as e:
            print(f"Failed to generate table report for project {pid}: {e}")
            return jsonify({'error': str(e)}), 500

        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_html)
        except Exception as e:
            print(f"Warning: could not write report file {report_path}: {e}")

        proj_dir = os.path.dirname(report_path)
        return send_from_directory(proj_dir, report_filename, mimetype='text/html')

    except Exception as e:
        return jsonify({'error': str(e)}), 500

def convert_routes_df_to_visualization_data(routes_df, pid: str | None = None):
    """ìµœì í™” ê²°ê³¼ DataFrameì„ visualizationìš© ë°ì´í„°ë¡œ ë³€í™˜"""
    route_data = {
        'routes': [],
        'total_distance': 'N/A',
        'total_duration': 'N/A',
        'route_count': 0,
        'optimization_score': 'N/A'
    }
    
    try:
        # ë¼ìš°íŠ¸ ê°œìˆ˜ ê³„ì‚°
        if 'Vehicle' in routes_df.columns:
            unique_vehicles = routes_df['Vehicle'].unique()
            route_data['route_count'] = len(unique_vehicles)
            
            # ê° ì°¨ëŸ‰ë³„ ë¼ìš°íŠ¸ ì •ë³´ êµ¬ì„±
            for vehicle_id in unique_vehicles:
                vehicle_routes = routes_df[routes_df['Vehicle'] == vehicle_id]
                
                route_info = {
                    'vehicle_id': int(vehicle_id),
                    'stops': [],
                    'geometry': None
                }
                
                # ì •ë¥˜ì¥ ì •ë³´ ì¶”ê°€
                for _, row in vehicle_routes.iterrows():
                    stop = {
                        'name': row.get('Location', f'Stop {len(route_info["stops"]) + 1}'),
                        'latitude': float(row.get('Lat', 0)),
                        'longitude': float(row.get('Lon', 0)),
                        'order': int(row.get('Order', len(route_info['stops'])))
                    }
                    route_info['stops'].append(stop)
                
                route_data['routes'].append(route_info)
        
        # í†µê³„ ì •ë³´ ê³„ì‚° (ìš”ì•½ íŒŒì¼ì´ ìˆìœ¼ë©´)
        if pid is None:
            pid = 'default'
        summary_file = project_path('optimization_summary.csv', pid)
        if os.path.exists(summary_file):
            summary_df = pd.read_csv(summary_file, encoding='utf-8-sig')
            if not summary_df.empty:
                total_dist = summary_df.get('Total_Distance_m', [0]).iloc[0]
                route_data['total_distance'] = f"{total_dist/1000:.1f} km" if total_dist > 0 else 'N/A'
                
                total_time = summary_df.get('Total_Time_s', [0]).iloc[0]
                route_data['total_duration'] = f"{total_time//60:.0f} min" if total_time > 0 else 'N/A'
                
                # ê°„ë‹¨í•œ ìµœì í™” ì ìˆ˜ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
                if total_dist > 0 and route_data['route_count'] > 0:
                    avg_dist_per_route = total_dist / route_data['route_count'] / 1000
                    score = max(0, 100 - avg_dist_per_route * 2)  # ì„ì‹œ ê³„ì‚°ì‹
                    route_data['optimization_score'] = f"{score:.0f}%"
                    
    except Exception as e:
        print(f"Error converting route data: {e}")
    
    return route_data



@app.route('/get-routes', methods=['GET'])
def get_routes():
    """ìŠ¤ë§ˆíŠ¸ ê²½ë¡œ ë¡œë”©: ìºì‹œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜, ì—†ìœ¼ë©´ ìƒì„±"""
    try:
        pid = get_project_id()
        # 1ï¸âƒ£ ìºì‹œëœ ê²½ë¡œ íŒŒì¼ í™•ì¸
        if os.path.exists(project_path('generated_routes.json', pid)):
            print("ğŸ“‚ ìºì‹œëœ ê²½ë¡œ ë°ì´í„° ë°œê²¬, ë¡œë”© ì¤‘...")
            
            try:
                with open(project_path('generated_routes.json', pid), 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                # ìºì‹œëœ vehicle_routesì— route_loadë‚˜ waypoint.demandê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ê°•
                vehicle_routes = cached_data.get('vehicle_routes', {})
                try:
                    if os.path.exists(project_path('optimization_routes.csv', pid)):
                        routes_df = pd.read_csv(project_path('optimization_routes.csv', pid), encoding='utf-8-sig')
                        routes_df['Vehicle_ID'] = pd.to_numeric(routes_df['Vehicle_ID'], errors='coerce')
                        routes_df['Stop_Order'] = pd.to_numeric(routes_df['Stop_Order'], errors='coerce')
                        routes_df = routes_df.dropna(subset=['Vehicle_ID', 'Stop_Order'])
                        # Load ì»¬ëŸ¼ ë³´ì¥
                        if 'Load' not in routes_df.columns and 'Cumulative_Load' in routes_df.columns:
                            try:
                                routes_df['Cumulative_Load'] = pd.to_numeric(routes_df['Cumulative_Load'], errors='coerce').fillna(0).astype(int)
                                routes_df['Load'] = 0
                                for vid, grp in routes_df.groupby('Vehicle_ID'):
                                    grp_sorted = grp.sort_values('Stop_Order').copy()
                                    prev = 0
                                    loads = []
                                    for _, r in grp_sorted.iterrows():
                                        loc_type = str(r.get('Location_Type', ''))
                                        cum = int(r.get('Cumulative_Load', 0) or 0)
                                        delta = 0 if loc_type == 'depot' else max(0, cum - prev)
                                        loads.append(delta)
                                        prev = cum
                                    routes_df.loc[grp_sorted.index, 'Load'] = loads
                            except Exception as _e:
                                print(f"Load ë³´ê°• ì‹¤íŒ¨(get_routes): {_e}")
                        # ì°¨ëŸ‰ë³„ ìµœì¢… Cumulative_Load ì‚¬ì „ ìƒì„±
                        final_load_by_vehicle = {}
                        for vid, grp in routes_df.groupby('Vehicle_ID'):
                            grp_sorted = grp.sort_values('Stop_Order')
                            try:
                                final_val = int(pd.to_numeric(grp_sorted['Cumulative_Load'], errors='coerce').dropna().iloc[-1])
                            except Exception:
                                final_val = 0
                            final_load_by_vehicle[int(vid)] = final_val
                        # ìºì‹œ ë°ì´í„° ë³´ê°•: route_load, waypoint.demand
                        def _normalize_name_local(s: str | None) -> str:
                            if s is None:
                                return ''
                            try:
                                return ' '.join(str(s).strip().lower().split())
                            except Exception:
                                return str(s).strip().lower()

                        for key, route in vehicle_routes.items():
                            vid = int(route.get('vehicle_id', key))
                            if 'route_load' not in route or route.get('route_load') in (None, 0):
                                route['route_load'] = final_load_by_vehicle.get(vid, 0)
                            # waypoint.demand ì£¼ì… (ID ìš°ì„ , ì´ë¦„ í´ë°±)
                            try:
                                grp = routes_df[routes_df['Vehicle_ID'] == vid].sort_values('Stop_Order').copy()
                                if 'waypoints' in route and isinstance(route['waypoints'], list):
                                    for idx, wp in enumerate(route['waypoints']):
                                        wp_name = str(wp.get('name', ''))
                                        wp_id = str(wp.get('id', '')) if wp.get('id') is not None else None
                                        wp_name_norm = _normalize_name_local(wp_name)
                                        load_val = 0
                                        # 1) Location_ID ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ IDë¡œ ìš°ì„  ë§¤ì¹­
                                        if 'Location_ID' in grp.columns and wp_id:
                                            matched = grp[grp['Location_ID'].astype(str) == wp_id]
                                            if not matched.empty:
                                                try:
                                                    load_val = int(matched.iloc[0].get('Load', 0) or 0)
                                                    wp['demand'] = load_val
                                                    continue
                                                except Exception:
                                                    load_val = 0
                                        # 2) ë™ì¼í•œ ìˆœì„œì˜ í›„ë³´ ì‚¬ìš©(ì¸ë±ìŠ¤ ê¸°ë°˜)
                                        candidate = grp.iloc[idx] if idx < len(grp) else None
                                        if candidate is not None:
                                            # IDê°€ ìˆìœ¼ë©´ ë¹„êµ
                                            if 'Location_ID' in candidate and pd.notna(candidate.get('Location_ID')) and wp_id:
                                                if str(candidate.get('Location_ID')) == wp_id:
                                                    load_val = int(candidate.get('Load', 0) or 0)
                                                    wp['demand'] = load_val
                                                    continue
                                            # ì´ë¦„ìœ¼ë¡œ ë¹„êµ (ì •ê·œí™”)
                                            cand_name_norm = _normalize_name_local(candidate.get('Location_Name', ''))
                                            if cand_name_norm and cand_name_norm == wp_name_norm:
                                                load_val = int(candidate.get('Load', 0) or 0)
                                                wp['demand'] = load_val
                                                continue
                                        # 3) ì´ë¦„ìœ¼ë¡œ ì „ì²´ ê²€ìƒ‰ í´ë°±
                                        if 'Location_Name' in grp.columns:
                                            # normalized match across the group
                                            try:
                                                matched = grp[grp['Location_Name'].astype(str).apply(lambda x: _normalize_name_local(x) == wp_name_norm)]
                                            except Exception:
                                                matched = grp[grp['Location_Name'].astype(str) == wp_name]
                                            if not matched.empty:
                                                try:
                                                    load_val = int(matched.iloc[0].get('Load', 0) or 0)
                                                except Exception:
                                                    load_val = 0
                                        wp['demand'] = load_val
                            except Exception as _wp_e:
                                print(f"waypoint.demand ë³´ê°• ì‹¤íŒ¨(V{vid}): {_wp_e}")
                except Exception as enrich_error:
                    print(f"âš ï¸ ìºì‹œ ë³´ê°• ì¤‘ ì˜¤ë¥˜(Load): {enrich_error}")

                print(f"âœ… ìºì‹œëœ ê²½ë¡œ ë¡œë“œ ì„±ê³µ: {len(cached_data.get('vehicle_routes', {}))}ê°œ ì°¨ëŸ‰")
                
                return jsonify({
                    'success': True,
                    'from_cache': True,
                    'generated_at': cached_data.get('generated_at'),
                    'vehicle_routes': vehicle_routes,
                    'statistics': cached_data.get('statistics', {})
                })
                
            except Exception as cache_error:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {cache_error}, T-mapìœ¼ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                # ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì•„ë˜ë¡œ ê³„ì† ì§„í–‰í•´ì„œ ìƒˆë¡œ ìƒì„±
        
        # 2ï¸âƒ£ ìºì‹œê°€ ì—†ìœ¼ë©´ T-map APIë¡œ ìƒˆë¡œ ìƒì„±
        print("ğŸš€ ìºì‹œëœ ë°ì´í„°ê°€ ì—†ì–´ì„œ T-map APIë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        return generate_routes_from_csv_internal()
        
    except Exception as e:
        print(f"âŒ ê²½ë¡œ ë¡œë”© ì‹¤íŒ¨: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/generate-routes-from-csv', methods=['POST'])
def generate_routes_from_csv():
    """ê°•ì œë¡œ T-map APIë¥¼ í˜¸ì¶œí•´ì„œ ìƒˆë¡œìš´ ê²½ë¡œ ìƒì„± (Refresh Routesìš©)
    ì„ íƒì  ì˜µì…˜(searchOption, carType, viaTime, startTime)ì„ JSON ë°”ë””ë¡œ ë°›ì•„ ì „ë‹¬í•œë‹¤.
    ëª¨ë‘ ë¬¸ìì—´ë¡œ ì²˜ë¦¬.
    """
    try:
        options = request.get_json(silent=True) or {}
    except Exception:
        options = {}
    return generate_routes_from_csv_internal(options)

def generate_routes_from_csv_internal(options: dict | None = None):
    """optimization_routes.csv íŒŒì¼ì„ ì½ì–´ì„œ T-mapìœ¼ë¡œ ì‹¤ì œ ê²½ë¡œ ìƒì„±"""
    try:
        pid = get_project_id()
        options = options or {}
        # ë¬¸ìì—´ë¡œ ë³´ì¥
        opt_search = options.get('searchOption')
        opt_car = options.get('carType')
        opt_via = options.get('viaTime')
        opt_start = options.get('startTime')
        if opt_search is not None:
            opt_search = str(opt_search)
        if opt_car is not None:
            opt_car = str(opt_car)
        if opt_via is not None:
            opt_via = str(opt_via)
        if opt_start is not None:
            opt_start = str(opt_start)
        
        print("ğŸš€ ê²½ë¡œ ìƒì„± ì‹œì‘...")
        
        # CSV íŒŒì¼ë“¤ ì½ê¸°
        if not os.path.exists(project_path('optimization_routes.csv', pid)):
            return jsonify({'error': 'optimization_routes.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        if not os.path.exists(project_path('locations.csv', pid)):
            return jsonify({'error': 'locations.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
            
        print("ğŸ“ CSV íŒŒì¼ ì½ê¸° ì¤‘...")

        routes_df = pd.read_csv(project_path('optimization_routes.csv', pid), encoding='utf-8-sig')
        locations_df = pd.read_csv(project_path('locations.csv', pid), encoding='utf-8-sig')
        
        # ë°ì´í„° íƒ€ì… ê°•ì œ ë³€í™˜
        routes_df['Vehicle_ID'] = pd.to_numeric(routes_df['Vehicle_ID'], errors='coerce')
        routes_df['Stop_Order'] = pd.to_numeric(routes_df['Stop_Order'], errors='coerce')
        # Load ì»¬ëŸ¼ì´ ì—†ì„ ê²½ìš° Cumulative_Load ì°¨ë¶„ìœ¼ë¡œ ìƒì„± (depot=0)
        if 'Load' not in routes_df.columns:
            try:
                routes_df['Cumulative_Load'] = pd.to_numeric(routes_df['Cumulative_Load'], errors='coerce').fillna(0).astype(int)
                routes_df['Load'] = 0
                for vid, grp in routes_df.groupby('Vehicle_ID'):
                    grp_sorted = grp.sort_values('Stop_Order').copy()
                    prev = 0
                    loads = []
                    for _, r in grp_sorted.iterrows():
                        loc_type = str(r.get('Location_Type', ''))
                        cum = int(r.get('Cumulative_Load', 0) or 0)
                        delta = 0 if loc_type == 'depot' else max(0, cum - prev)
                        loads.append(delta)
                        prev = cum
                    routes_df.loc[grp_sorted.index, 'Load'] = loads
            except Exception as _e:
                print(f"Load ì»¬ëŸ¼ ìƒì„± ì‹¤íŒ¨: {_e}")
        locations_df['lon'] = pd.to_numeric(locations_df['lon'], errors='coerce')
        locations_df['lat'] = pd.to_numeric(locations_df['lat'], errors='coerce')
        
        # NaN ê°’ ì œê±°
        routes_df = routes_df.dropna(subset=['Vehicle_ID', 'Stop_Order'])
        locations_df = locations_df.dropna(subset=['lon', 'lat'])
        
        print(f"ğŸ“Š Routes ë°ì´í„°: {len(routes_df)}í–‰, Locations ë°ì´í„°: {len(locations_df)}í–‰")
        
        # ìœ„ì¹˜ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ID ìš°ì„  ì¡°íšŒ, name í´ë°±)
        # location_by_id: id -> location dict
        # location_by_name: name -> location dict (í´ë°±ìš©, ì¤‘ë³µ ì‹œ ì²« í•­ëª© ì‚¬ìš©)
        location_by_id = {}
        location_by_name = {}
        def _normalize_name(s: str | None) -> str:
            if s is None:
                return ''
            try:
                # strip, lowercase, collapse whitespace
                return ' '.join(str(s).strip().lower().split())
            except Exception:
                return str(s).strip().lower()
        for _, row in locations_df.iterrows():
            try:
                loc_id = str(row['id']) if 'id' in row and row['id'] is not None else None
                loc_name = str(row['name'])
                loc_entry = {
                    'id': loc_id,
                    'name': loc_name,
                    'x': float(row['lon']),  # T-map APIëŠ” x=ê²½ë„, y=ìœ„ë„
                    'y': float(row['lat'])
                }
                if loc_id:
                    location_by_id[loc_id] = loc_entry
                # nameì€ ì¤‘ë³µ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‚˜ í´ë°± ìš©ë„ë¡œ ì²« ë§¤ì¹˜ë§Œ ì‚¬ìš©
                norm = _normalize_name(loc_name)
                if norm and norm not in location_by_name:
                    location_by_name[norm] = loc_entry
            except (ValueError, TypeError) as e:
                print(f"ìœ„ì¹˜ ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜: {row.get('name', '')} - {e}")
                continue

        print(f"ğŸ“ ìœ„ì¹˜ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ: {len(location_by_id)}ê°œ id, {len(location_by_name)}ê°œ name(í´ë°±)")
        
        # T-map ë¼ìš°í„° ì´ˆê¸°í™”
        tmap_router = TmapRoute()
        
        # ì°¨ëŸ‰ë³„ë¡œ ê²½ë¡œ ìƒì„±
        vehicle_routes = {}
        unique_vehicles = routes_df['Vehicle_ID'].unique()
        # depot ì´ë¦„ì„ locations.csvì˜ ì²« í–‰ì—ì„œ ê°€ì ¸ì˜´
        try:
            depot_name_from_csv = None
            if not locations_df.empty:
                depot_row = locations_df.sort_values('id').iloc[0]
                depot_name_from_csv = str(depot_row['name'])
        except Exception as _de:
            print(f"Depot ì¶”ë¡  ì‹¤íŒ¨: {_de}")
        
        for vehicle_id in unique_vehicles:
            try:
                vehicle_id = int(vehicle_id)  # í™•ì‹¤íˆ ì •ìˆ˜ë¡œ ë³€í™˜
                vehicle_data = routes_df[routes_df['Vehicle_ID'] == vehicle_id].copy()
                vehicle_data = vehicle_data.sort_values('Stop_Order')

                # ì°¨ëŸ‰ë³„ ìµœì¢… ëˆ„ì  Load ê³„ì‚° (optimization_routes.csvì˜ ë§ˆì§€ë§‰ Cumulative_Load ê°’)
                try:
                    final_cum_load = int(pd.to_numeric(vehicle_data['Cumulative_Load'], errors='coerce').dropna().iloc[-1]) if not vehicle_data.empty else 0
                except Exception:
                    final_cum_load = 0
                
                # ìµœì í™” ê²°ê³¼ì˜ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì²«=ì¶œë°œ, ë§ˆì§€ë§‰=ë„ì°©, ë‚˜ë¨¸ì§€=ê²½ìœ )
                if vehicle_data.empty or len(vehicle_data) < 2:
                    print(f"Vehicle {vehicle_id}: ìœ íš¨í•œ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                start_row = vehicle_data.iloc[0]
                end_row = vehicle_data.iloc[-1]
                start_name = str(start_row['Location_Name'])
                end_name = str(end_row['Location_Name'])
                via_names = [str(n) for n in vehicle_data.iloc[1:-1]['Location_Name'].tolist()]
                # ëª¨ë“œ ë©”íƒ€ë°ì´í„° ê³„ì‚°(ì˜µì…˜): depot ì‹œì‘/ì¢…ë£Œ ì—¬ë¶€ë¡œ ì¶”ë¡ 
                inferred_mode = 'FREE_START_DEPOT_END'
                if str(start_row['Location_Type']) == 'depot' and str(end_row['Location_Type']) == 'waypoint':
                    inferred_mode = 'DEPOT_START_OPEN_END'
                elif str(start_row['Location_Type']) == 'waypoint' and str(end_row['Location_Type']) == 'depot':
                    inferred_mode = 'FREE_START_DEPOT_END'

                # optimization_routes.csvì—ì„œ Location_ID ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                # start_row, end_row, via_names ê°ì²´ì—ì„œ Location_IDê°€ ìˆì„ ê°€ëŠ¥ì„± ê³ ë ¤
                def extract_id_from_row(row):
                    if 'Location_ID' in row and pd.notna(row.get('Location_ID')):
                        return str(row.get('Location_ID'))
                    return None

                start_id_candidate = extract_id_from_row(start_row)
                end_id_candidate = extract_id_from_row(end_row)
                via_id_candidates = [extract_id_from_row(r) for _, r in vehicle_data.iloc[1:-1].iterrows()]

                # ìœ„ì¹˜ ì •ë³´ í™•ì¸ (ID ìš°ì„ , name í´ë°±)
                def exists_in_locations(id_or_name):
                    if id_or_name is None:
                        return False
                    s = str(id_or_name)
                    # ID exact match
                    if s in location_by_id:
                        return True
                    # name normalized match
                    if _normalize_name(s) in location_by_name:
                        return True
                    return False

                if not exists_in_locations(start_id_candidate or start_name):
                    print(f"Vehicle {vehicle_id}: ì¶œë°œì§€ '{start_id_candidate or start_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                if not exists_in_locations(end_id_candidate or end_name):
                    print(f"Vehicle {vehicle_id}: ë„ì°©ì§€ '{end_id_candidate or end_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                missing = [n for i, n in enumerate(via_names) if not exists_in_locations(via_id_candidates[i] if i < len(via_id_candidates) else n)]
                if missing:
                    print(f"Vehicle {vehicle_id}: ê²½ìœ ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {missing}")
                    continue
                
                # start/end/viaë¥¼ ID ìš°ì„ ìœ¼ë¡œ ì°¾ê³ , ì—†ìœ¼ë©´ name í´ë°±
                def resolve_location_by_id_or_name(name_or_id):
                    # ìš°ì„  idë¡œ ê²€ìƒ‰
                    if name_or_id is None:
                        return None
                    s = str(name_or_id)
                    if s in location_by_id:
                        return location_by_id[s].copy()
                    norm = _normalize_name(s)
                    if norm in location_by_name:
                        return location_by_name[norm].copy()
                    return None

                start_point = resolve_location_by_id_or_name(start_id_candidate or start_name)
                end_point = resolve_location_by_id_or_name(end_id_candidate or end_name)
                via_points = []
                for i, name in enumerate(via_names):
                    candidate_id = via_id_candidates[i] if i < len(via_id_candidates) else None
                    vp = resolve_location_by_id_or_name(candidate_id or name)
                    via_points.append(vp)
                # demand ì£¼ì…: routes_dfì˜ Load ê°’ì„ name ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ (ë™ì¼ ì´ë¦„ ë‹¤íšŒ ì¶œí˜„ ê°€ëŠ¥ì„± ë‚®ë‹¤ê³  ê°€ì •)
                try:
                    # name -> Load ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ë¥¼ Stop_Order ìˆœì„œë¡œ ë½‘ì•„ëƒ„
                    loads_seq = vehicle_data[['Location_Name', 'Location_Type', 'Load']].to_dict('records')
                    # ì¶œë°œì§€/ê²½ìœ ì§€/ë„ì°©ì§€ ê°ê°ì— demand ì„¸íŒ…
                    def find_first_load(name, loc_type):
                        for rec in loads_seq:
                            if str(rec['Location_Name']) == str(name) and str(rec.get('Location_Type', '')) == str(loc_type):
                                return int(rec.get('Load', 0) or 0)
                        # íƒ€ì…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì´ë¦„ë§Œ ë§¤ì¹­í•˜ëŠ” í´ë°±
                        for rec in loads_seq:
                            if str(rec['Location_Name']) == str(name):
                                return int(rec.get('Load', 0) or 0)
                        return 0
                    # ì¶œë°œì§€(depot or waypoint)
                    start_point['demand'] = find_first_load(start_name, vehicle_data.iloc[0].get('Location_Type', ''))
                    # ê²½ìœ ì§€ë“¤(waypoint)
                    for i, vp in enumerate(via_points):
                        loc_name = via_names[i]
                        # í•´ë‹¹ viaì˜ íƒ€ì…ì€ ë³´í†µ waypoint
                        vp['demand'] = find_first_load(loc_name, 'waypoint')
                    # ë„ì°©ì§€(depot or waypoint)
                    end_point['demand'] = find_first_load(end_name, vehicle_data.iloc[-1].get('Location_Type', ''))
                except Exception as _inject_e:
                    print(f"demand ì£¼ì… ì¤‘ ì˜¤ë¥˜(V{vehicle_id}): {_inject_e}")
                
            except (ValueError, TypeError) as e:
                print(f"Vehicle ID ë³€í™˜ ì˜¤ë¥˜: {vehicle_id} - {e}")
                continue
            
            # T-map APIë¡œ ê²½ë¡œ ìƒì„±
            try:
                print(f"Vehicle {vehicle_id} ê²½ë¡œ ìƒì„± ì¤‘...")
                print(f"  ì¶œë°œì§€: {start_point['name']}")
                print(f"  ë„ì°©ì§€: {end_point['name']}")
                print(f"  ê²½ìœ ì§€: {len(via_points)}ê°œ")
                
                # If there are intermediate via points, call T-map sequential route API.
                # If there are no via points (direct from start -> end), some T-map
                # endpoints can fail or return no geometry. In that case we create a
                # simple fallback single-leg route (LineString between start and end)
                # and estimate distance/time so the vehicle still has a usable result.
                if via_points:
                    route_result = tmap_router.get_route(
                        start_point,
                        end_point,
                        via_points,
                        searchOption=opt_search,
                        start_time=opt_start,
                        carType=opt_car,
                        viaTime=opt_via
                    )
                else:
                    # Try OSRM for single-leg routing first (road network based).
                    try:
                        route_result = tmap_router.get_route_single(start_point, end_point)
                    except Exception as e_single:
                        print(f"  âš ï¸ T-map ë‹¨ì¼ ê²½ë¡œ API ì‹¤íŒ¨, í•˜ë²„ì‚¬ì¸ í´ë°± ì‚¬ìš©: {e_single}")
                        # Haversine fallback
                        try:
                            sx, sy = float(start_point['x']), float(start_point['y'])
                            ex, ey = float(end_point['x']), float(end_point['y'])
                        except Exception:
                            raise ValueError(f"Invalid coordinate data for vehicle {vehicle_id}")

                        from math import radians, sin, cos, atan2, sqrt

                        def haversine_meters(lon1, lat1, lon2, lat2):
                            R = 6371000.0
                            dlat = radians(lat2 - lat1)
                            dlon = radians(lon2 - lon1)
                            a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2
                            c = 2 * atan2(sqrt(a), sqrt(1-a))
                            return R * c

                        dist_m = haversine_meters(sx, sy, ex, ey)
                        avg_speed_kmh = 40.0
                        time_s = (dist_m / 1000.0) / avg_speed_kmh * 3600.0 if dist_m > 0 else 0

                        route_result = {
                            'features': [
                                {
                                    'type': 'Feature',
                                    'geometry': {
                                        'type': 'LineString',
                                        'coordinates': [[sx, sy], [ex, ey]]
                                    },
                                    'properties': {}
                                }
                            ],
                            'properties': {
                                'totalDistance': dist_m,
                                'totalTime': time_s
                            }
                        }
                
                # ê²½ë¡œ ë°ì´í„° ì²˜ë¦¬
                if 'features' in route_result and route_result['features']:
                    # ê²½ë¡œ ì¢Œí‘œ ì¶”ì¶œ
                    route_coordinates = []
                    for feature in route_result['features']:
                        if feature['geometry']['type'] == 'LineString':
                            coords = feature['geometry']['coordinates']
                            route_coordinates.extend(coords)
                    
                    # ì¤‘ë³µ ì¢Œí‘œ ì œê±°
                    unique_coords = []
                    for coord in route_coordinates:
                        if not unique_coords or coord != unique_coords[-1]:
                            unique_coords.append(coord)
                    
                    # ì‹¤ì œ ë°©ë¬¸ ìˆœì„œ: ì¶œë°œì§€ -> ê²½ìœ ì§€ë“¤ -> ë„ì°©ì§€
                    all_waypoints = [start_point] + via_points + [end_point]
                    
                    # ì°¨ëŸ‰ ê²½ë¡œ ì •ë³´ ì €ì¥
                    vehicle_routes[str(vehicle_id)] = {
                        'vehicle_id': int(vehicle_id),
                        'start_point': start_point,
                        'end_point': end_point,
                        'via_points': via_points,
                        'waypoints': all_waypoints,  # ì‹¤ì œ ë°©ë¬¸ ìˆœì„œëŒ€ë¡œ
                        'route_geometry': {
                            'type': 'LineString',
                            'coordinates': unique_coords
                        },
                        'properties': route_result.get('properties', {}),
                        'total_distance': float(route_result.get('properties', {}).get('totalDistance', 0)),
                        'total_time': float(route_result.get('properties', {}).get('totalTime', 0)),
                        # ëˆ„ì  Loadì˜ ìµœì¢…ê°’ (ì°¨ëŸ‰ë³„)
                        'route_load': final_cum_load
                    }
                    
                    print(f"  âœ… ì„±ê³µ: {len(unique_coords)}ê°œ ì¢Œí‘œ")
                    
                else:
                    print(f"  âŒ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: ì‘ë‹µì— ê²½ë¡œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as route_error:
                print(f"  âŒ Vehicle {vehicle_id} ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: {route_error}")
                continue
        
        # í†µê³„ ì •ë³´ ê³„ì‚°
        try:
            total_distance = sum(
                float(route.get('total_distance', 0)) 
                for route in vehicle_routes.values()
            )
            total_time = sum(
                float(route.get('total_time', 0)) 
                for route in vehicle_routes.values()
            )
        except (ValueError, TypeError) as e:
            print(f"í†µê³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            total_distance = 0
            total_time = 0
        
        response_data = {
            'success': True,
            'vehicle_routes': vehicle_routes,
            'statistics': {
                'total_distance': f"{total_distance/1000:.1f} km" if total_distance > 0 else "N/A",
                'total_time': f"{int(total_time//60):.0f} min" if total_time > 0 else "N/A",
                'route_count': len(vehicle_routes),
                'optimization_score': f"{min(100, max(0, 100 - len(vehicle_routes) * 5)):.0f}%"
            }
        }
        
        # ğŸ“ ê²½ë¡œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ì— ì €ì¥
        try:
            cache_data = {
                'generated_at': datetime.now().isoformat(),
                'vehicle_routes': vehicle_routes,
                'statistics': response_data['statistics'],
                'source_csv': {
                    'routes_file': 'optimization_routes.csv',
                    'locations_file': 'locations.csv'
                },
                'inferred_route_mode': inferred_mode if 'inferred_mode' in locals() else 'unknown'
            }
            
            with open(project_path('generated_routes.json', pid), 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'last_generated': datetime.now().isoformat(),
                'route_count': len(vehicle_routes),
                'total_distance_m': total_distance,
                'total_time_s': total_time
            }
            
            with open(project_path('route_metadata.json', pid), 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
            print(f"ğŸ’¾ ê²½ë¡œ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {project_path('generated_routes.json', pid)}")
            # If a per-project HTML report exists, remove it so next View Report regenerates it
            try:
                report_file = project_path('route_report.html', pid)
                if os.path.exists(report_file):
                    os.remove(report_file)
                    print(f"ğŸ—‘ï¸ Removed stale report for project {pid}: {report_file}")
                # also remove table report so it will be regenerated on next View Report
                try:
                    table_report = project_path('route_table_report.html', pid)
                    if os.path.exists(table_report):
                        os.remove(table_report)
                        print(f"ğŸ—‘ï¸ Removed stale table report for project {pid}: {table_report}")
                except Exception as rm_table_err:
                    print(f"âš ï¸ Failed to remove stale table report for project {pid}: {rm_table_err}")
            except Exception as rm_err:
                print(f"âš ï¸ Failed to remove stale report for project {pid}: {rm_err}")
            
        except Exception as save_error:
            print(f"âš ï¸ ê²½ë¡œ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {save_error}")
            # ì €ì¥ ì‹¤íŒ¨í•´ë„ ì‘ë‹µì€ ì •ìƒì ìœ¼ë¡œ ë°˜í™˜
        
        print(f"âœ… ì´ {len(vehicle_routes)}ê°œ ì°¨ëŸ‰ ê²½ë¡œ ìƒì„± ì™„ë£Œ")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"âŒ ê²½ë¡œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: {str(e)}'}), 500

@app.route('/check-route-cache', methods=['GET'])
def check_route_cache():
    """ê²½ë¡œ ìºì‹œ ìƒíƒœ í™•ì¸"""
    try:
        pid = get_project_id()
        if os.path.exists(project_path('generated_routes.json', pid)) and os.path.exists(project_path('route_metadata.json', pid)):
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ìƒì„± ì‹œê°„ í™•ì¸
            with open(project_path('route_metadata.json', pid), 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            return jsonify({
                'has_cache': True,
                'generated_at': metadata.get('last_generated'),
                'route_count': metadata.get('route_count', 0),
                'message': 'ìºì‹œëœ ê²½ë¡œ ë°ì´í„° ìˆìŒ'
            })
        else:
            return jsonify({
                'has_cache': False,
                'message': 'ìºì‹œëœ ê²½ë¡œ ë°ì´í„° ì—†ìŒ'
            })
            
    except Exception as e:
        return jsonify({
            'has_cache': False,
            'message': f'ìºì‹œ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}'
        })

if __name__ == '__main__':
    app.run(debug=True)
    # app.run(debug=True,host='192.168.0.114', port=5000)
